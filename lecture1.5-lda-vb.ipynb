{
 "metadata": {
  "name": "",
  "signature": "sha256:f967fab700fa287ee4dbfc866e6b7046082a2a31c1f024a959bce7308cd9ed69"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Variational Inference for LDA\n",
      "=============================\n",
      "\n",
      "In this lecture, we discuss the variational inference for LDA.\n",
      "\n",
      "\n",
      "Variational inference in general\n",
      "--------------------------------\n",
      "\n",
      "Let's assume that we have a Bayesian model that has a set of observation $X$, latent variables $Z$, and parameters of latent variables $\\Phi$. We are usually intesrested in the posterior distribution of this model\n",
      "\n",
      "$$\n",
      "p(Z|X,\\Phi) = \\frac{p(X|Z)p(Z|\\Phi)}{p(X | \\Phi)} \\\\\n",
      "= \\frac{p(X|Z)p(Z|\\Phi)}{\\int p(X, Z| \\Phi) dZ}.\n",
      "$$\n",
      "\n",
      "However, for many interesting models, the posterior is intractable due to the normalization constant $P(X|\\Phi)$.\n",
      "\n",
      "The key idea of variational inference is to approximate the posterior of latent variables by introducing variational distribution over the latent variables with its own variational parameters\n",
      "\n",
      "$$\n",
      "p(Z|X,\\Phi) \\approx q(Z|\\Psi),\n",
      "$$\n",
      "\n",
      "where $\\Psi$ is a set of parameters for the variational distribution $q$.\n",
      "\n",
      "Then, we minimize the difference between true posterior and variational distribution in terms of KL-divergence.\n",
      "\n",
      "$$\n",
      "KL(q||p) = \\sum_Z q(Z)\\log\\frac{q(Z)}{p(Z|X)} \\\\\n",
      "= \\sum_Z q(Z)\\log\\frac{q(Z)}{p(Z,X)} - \\log p(X) \\\\\n",
      "= E_q[\\log\\frac{q(Z)}{p(Z,X)}] - \\log p(X) \\\\\n",
      "$$\n",
      "\n",
      "(we omit the parameters $\\Phi$ and $\\Psi$ for notational simplicity) The first term is called negative ELBO (Evidence Lower BOund), and \n",
      "we can ignore the second term since that term has no relationship with the variational distribution. Therefore, minimizing KL divergence is equal to maximizing ELBO $(E_q[\\log p(Z,X)] - E_q[\\log q(Z)])$.\n",
      "\n",
      "To maximize ELBO, we can use several optimization techniques such as coodinate ascent or newton methods.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Variational inference for LDA\n",
      "-----------------------------\n",
      "\n",
      "Now we discuss how the variational inference can be applied to LDA model. In this section, we use the original LDA model of which joint likelihood is:\n",
      "\n",
      "$$p(x,z,\\theta|\\alpha,\\beta) = p(x|z,\\beta) p(z|\\theta) p(\\theta|\\alpha).$$\n",
      "\n",
      "We place fully factorized variational distributions over latent variable $z$ and $\\theta$:\n",
      "\n",
      "$$q(z|\\phi) = \\prod_{dn} q(z_{dn}|\\phi_{dn})$$\n",
      "$$q(\\theta|\\gamma) = \\prod_{d} q(\\theta_{d}|\\gamma_{d}).$$\n",
      "\n",
      "Then, the ELBO is\n",
      "\n",
      "$$\\text{ELBO}(\\phi,\\gamma) = E_q[\\log p(\\theta|\\alpha)] + E_q[\\log p(z|\\theta)] + E_q[\\log p(w|z,\\beta)]\\\\\n",
      "-E_q[\\log q(\\theta|\\gamma)] - E_q[\\log q(z|\\phi)]\n",
      "$$\n",
      "\n",
      "We can obtain the update rule for $\\phi$ by taking the partial derivative with respect to $\\phi_{dnk}$\n",
      "(Let's derive the update rule!!!)\n",
      "\n",
      "$$\\phi_{dnk} \\propto \\beta_{kv} \\exp (\\Psi(\\gamma_k) - \\Psi(\\sum_k \\gamma_k)).$$\n",
      "\n",
      "and obtain the update rule for $\\gamma$ by taking the partial derivative with respect to $\\gamma_{dk}$\n",
      "(Let's derive the update rule!!!)\n",
      "\n",
      "$$\\gamma_{dk} = \\alpha_k + \\sum \\phi_{dnk}$$\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read sample corpus from nltk.corpus.brown corpus\n",
      "# install nltk package, import nltk, and run nltk.download() to get corpora provided by nltk\n",
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "from nltk.corpus import stopwords\n",
      "from scipy.special import gammaln\n",
      "\n",
      "st = set(stopwords.words())\n",
      "st.add(u'.')\n",
      "st.add(u',')\n",
      "st.add(u'\\'\\'')\n",
      "st.add(u'``')\n",
      "st.add(u':')\n",
      "st.add(u'--')\n",
      "\n",
      "ndoc = 500\n",
      "\n",
      "_docs = brown.sents()\n",
      "docs = list()\n",
      "for di in xrange(ndoc):\n",
      "    doc = _docs[di]\n",
      "    new_doc = list()\n",
      "    for word in doc:\n",
      "        if word.lower() not in st:\n",
      "            new_doc.append(word.lower())\n",
      "    docs.append(new_doc)\n",
      "    \n",
      "# construct vocabulary\n",
      "_voca = set()\n",
      "for doc in docs:\n",
      "    _voca = _voca.union(set(doc))\n",
      "    \n",
      "nvoca = len(_voca)\n",
      "voca = dict()\n",
      "\n",
      "for word in _voca:\n",
      "    voca[word] = len(voca)\n",
      "voca_list = np.array(list(_voca))\n",
      "\n",
      "# convert word list to vector\n",
      "word_ids = list()   # word appearance\n",
      "word_cnt = list()   # word count\n",
      "for doc in docs:\n",
      "    ids = np.zeros(nvoca, dtype=int)\n",
      "    cnt = np.zeros(nvoca, dtype=int)\n",
      "    for word in doc:\n",
      "        ids[voca[word]] = 1\n",
      "        cnt[voca[word]] += 1\n",
      "    word_ids.append(ids)\n",
      "    word_cnt.append(cnt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set parameters\n",
      "alpha = 0.1\n",
      "ntopics = 10\n",
      "eps = 1e-100\n",
      "\n",
      "# initialize topic\n",
      "beta = np.random.dirichlet([0.1]*nvoca, size=ntopics)\n",
      "# initialize variational parameters\n",
      "gamma = np.random.dirichlet([1]*ntopics, size=ndoc)\n",
      "\n",
      "# accumulate phi for updateing beta\n",
      "sstat = np.zeros([ntopics, nvoca]) + eps"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 112
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.special import gammaln, psi\n",
      "\n",
      "#make it simple!\n",
      "max_iter = 100\n",
      "for it in xrange(max_iter):\n",
      "    for di in xrange(ndoc):    \n",
      "        phi = beta[:, word_ids[di]] * np.exp(psi(gamma[di,:]) - psi(np.sum(gamma[di,:])))[:,np.newaxis]\n",
      "        phi[:,word_ids[di]] /= np.sum(phi[:,word_ids[di]],0)\n",
      "        phi *= word_cnt[di]\n",
      "        gamma[di,:] = np.sum(phi,1) + alpha\n",
      "        sstat += phi\n",
      "    beta = sstat\n",
      "    sstat = np.zeros([ntopics, nvoca]) + eps    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print top words for each topic\n",
      "for topic in xrange(ntopics):\n",
      "    print 'topic %d : %s' % (topic, ' '.join(voca_list[np.argsort(beta[topic,:])[::-1][0:10]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 0 : even communists limited nato despite alliance today capital heart line\n",
        "topic 1 : said would state president mr. city administration year one jury\n",
        "topic 2 : said would state president mr. city administration year one jury\n",
        "topic 3 : said would state president mr. city administration year one jury\n",
        "topic 4 : said would state president mr. city administration year one jury\n",
        "topic 5 : said would state president mr. city administration year one jury\n",
        "topic 6 : limited provide needs community essential basis vital volume shopping operate\n",
        "topic 7 : said would state president mr. city administration year one jury\n",
        "topic 8 : limited two cases adc problems reported community martin employment discrimination\n",
        "topic 9 : said would state president mr. city administration year one jury\n"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Are there any problem with above code? Let's improve it!\n",
      "\n",
      "and derive the variational algorithm for smoothed LDA (place prior over $\\beta$)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}