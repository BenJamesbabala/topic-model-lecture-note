{
 "metadata": {
  "name": "",
  "signature": "sha256:02b107ec537382f9de9a8386c03ac6a3bb0e4e2388bf0cb0a122d57191fbd1e0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "LDA's generative processs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ntopic = 10\n",
      "nvoca = 20\n",
      "alpha = np.ones(ntopic)*0.1\n",
      "beta = np.ones(nvoca)*0.01\n",
      "theta = np.random.dirichlet(alpha)\n",
      "phi = np.random.dirichlet(beta, size=ntopic)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_z = np.random.multinomial(1, theta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = _z.nonzero()[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_w = np.random.multinomial(1, phi[z,:])\n",
      "w = _w.nonzero()[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "18"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Collapsed Gibbs Sampling\n",
      "========================\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read sample corpus from nltk.corpus.brown corpus\n",
      "# install nltk package, import nltk, and run nltk.download() to get corpora provided by nltk\n",
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "from nltk.corpus import stopwords\n",
      "from scipy.special import gammaln\n",
      "\n",
      "alpha = 0.1\n",
      "beta = 0.01\n",
      "\n",
      "ndoc = 500\n",
      "ntopics = 10\n",
      "\n",
      "st = set(stopwords.words())\n",
      "st.add(u'.')\n",
      "st.add(u',')\n",
      "st.add(u'\\'\\'')\n",
      "st.add(u'``')\n",
      "st.add(u':')\n",
      "st.add(u'--')\n",
      "\n",
      "_docs = brown.sents()\n",
      "docs = list()\n",
      "for di in xrange(ndoc):\n",
      "    doc = _docs[di]\n",
      "    new_doc = list()\n",
      "    for word in doc:\n",
      "        if word.lower() not in st:\n",
      "            new_doc.append(word.lower())\n",
      "    docs.append(new_doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# construct vocabulary\n",
      "_voca = set()\n",
      "for doc in docs:\n",
      "    _voca = _voca.union(set(doc))\n",
      "    \n",
      "nvoca = len(_voca)\n",
      "voca = dict()\n",
      "\n",
      "for word in _voca:\n",
      "    voca[word] = len(voca)\n",
      "voca_list = np.array(list(_voca))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set sampling variables\n",
      "\n",
      "word_topic = np.zeros([nvoca,ntopic])\n",
      "topic_sum = np.zeros(ntopic)\n",
      "doc_sum = np.zeros([ndoc, ntopic])\n",
      "\n",
      "assigned_topics = [list() for i in xrange(ndoc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#initial sampling process\n",
      "\n",
      "for di in xrange(ndoc):\n",
      "    doc = docs[di]\n",
      "    for word in doc:\n",
      "        w_no = voca[word]\n",
      "        prob = np.zeros(ntopic)\n",
      "        for topic in xrange(ntopic):\n",
      "            prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + doc_sum[di,topic])\n",
      "        prob /= np.sum(prob)\n",
      "        \n",
      "        # draw random sample\n",
      "        new_topic = np.random.multinomial(1, prob).nonzero()[0][0]\n",
      "        \n",
      "        assigned_topics[di].append(new_topic)\n",
      "        doc_sum[di,new_topic] += 1\n",
      "        topic_sum[new_topic] += 1\n",
      "        word_topic[w_no,new_topic] += 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# iterate sampling process\n",
      "\n",
      "niter = 100\n",
      "for it in xrange(niter):\n",
      "    for di in xrange(ndoc):\n",
      "        doc = docs[di]\n",
      "        for wi in xrange(len(doc)):\n",
      "            word = doc[wi]\n",
      "            w_no = voca[word]\n",
      "            prev_topic = assigned_topics[di][wi]\n",
      "\n",
      "            doc_sum[di,prev_topic] -= 1\n",
      "            topic_sum[prev_topic] -= 1\n",
      "            word_topic[w_no,prev_topic] -= 1\n",
      "\n",
      "            prob = np.zeros(ntopic)\n",
      "            for topic in xrange(ntopic):\n",
      "                prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + doc_sum[di,topic])\n",
      "            prob /= np.sum(prob)\n",
      "            \n",
      "            # draw random sample\n",
      "            new_topic = np.random.multinomial(1, prob).nonzero()[0][0]\n",
      "\n",
      "            assigned_topics[di][wi] = new_topic\n",
      "            doc_sum[di,new_topic] += 1\n",
      "            topic_sum[new_topic] += 1\n",
      "            word_topic[w_no,new_topic] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print top probability words for each topic\n",
      "for topic in xrange(ntopic):\n",
      "    print 'topic %d : %s' % (topic, ' '.join(voca_list[np.argsort(word_topic[:,topic])[::-1][0:10]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 0 : president federal million program year new state funds grants increase\n",
        "topic 1 : texas ) ( made college j. expected told statements austin\n",
        "topic 2 : jury committee county court said election sunday fulton resolution general\n",
        "topic 3 : would said mr. one charter chairman public go school day\n",
        "topic 4 : would home law days must rule proposed board bills legislature\n",
        "topic 5 : said council last republican mr. party eisenhower night even nato\n",
        "topic 6 : house bill dallas senate passed washington sen. monday race former\n",
        "topic 7 : administration would united laos states time aid one asked state\n",
        "topic 8 : said city state take cases director two hawksley campaign persons\n",
        "topic 9 : plan department care tax 1 last medical bonds would work\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Computing Heldout Likelihood\n",
      "============================\n",
      "\n",
      "Once you infer topics from corpus, you need to validate the performance of our model. Computing heldout likelihood is the easiest way to compare your model with others. Typically, it consists of four steps:\n",
      "\n",
      "1. Prepare test set documents\n",
      "2. Split each test document into two parts\n",
      "3. Infer topic proportion of test documents from the first half by using sampling\n",
      "4. Compute heldout likelihood on the second half\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 1\n",
      "------\n",
      "Prepare test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_ndoc = 100\n",
      "test_docs = list()\n",
      "\n",
      "for di in xrange(test_ndoc):\n",
      "    doc = _docs[di]\n",
      "    new_doc = list()\n",
      "    for word in doc:\n",
      "        if word.lower() not in st:\n",
      "            new_doc.append(word.lower())\n",
      "    test_docs.append(new_doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 2\n",
      "------\n",
      "\n",
      "Split each test document into two parts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_first = list()\n",
      "test_second = list()\n",
      "for doc in test_docs:\n",
      "    first_half = list()\n",
      "    second_half = list()\n",
      "    #split it your way!\n",
      "    for wi in xrange(len(doc)):\n",
      "        word = doc[wi]\n",
      "        if voca.has_key(word):\n",
      "            if wi%2 == 0:\n",
      "                first_half.append(doc[wi])\n",
      "            else:\n",
      "                second_half.append(doc[wi])\n",
      "    test_first.append(first_half)\n",
      "    test_second.append(second_half)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 3\n",
      "------\n",
      "\n",
      "Infer topic proportion of test documents from the first half by using sampling"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# iterate sampling process\n",
      "# note that this time we do not assign new words to word_topic matrix\n",
      "\n",
      "#topic_sum = np.zeros(ntopic) # Jooyeon : topic_sum is identical for training and testing\n",
      "t_doc_sum = np.zeros([test_ndoc, ntopic])\n",
      "\n",
      "t_assigned_topics = [list() for i in xrange(test_ndoc)]\n",
      "\n",
      "# Jooyeon : t_doc_sum and t_assigned_topics need to be initialized.\n",
      "\n",
      "niter = 100\n",
      "for it in xrange(niter):\n",
      "    for di in xrange(test_ndoc):\n",
      "        doc = test_first[di]\n",
      "        for wi in xrange(len(doc)):\n",
      "            word = doc[wi]\n",
      "            w_no = voca[word]\n",
      "            #prev_topic = assigned_topics[di][wi]\n",
      "            prev_topic = t_assigned_topics[di][wi]  # Jooyeon : changed to t_assigned_topics\n",
      "\n",
      "            if it != 0:\n",
      "                t_doc_sum[di,prev_topic] -= 1\n",
      "            #topic_sum[prev_topic] -= 1\n",
      "            #word_topic[w_no,prev_topic] -= 1\n",
      "\n",
      "            prob = np.zeros(ntopic)\n",
      "            for topic in xrange(ntopic):\n",
      "                prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + t_doc_sum[di,topic])\n",
      "            prob /= np.sum(prob)\n",
      "\n",
      "            # draw random sample\n",
      "            new_topic = np.random.multinomial(1, prob).nonzero()[0][0]\n",
      "\n",
      "            #assigned_topics[di][wi] = new_topic\n",
      "            t_assigned_topics[di][wi] = new_topic # Jooyeon : changed to t_assigned_topics\n",
      "            t_doc_sum[di,new_topic] += 1\n",
      "            #topic_sum[new_topic] += 1\n",
      "            #word_topic[w_no,new_topic] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 4\n",
      "------\n",
      "\n",
      "Compute per-word heldout log likelihood on the second half"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heldout_ll = 0.\n",
      "cnt = 0.\n",
      "\n",
      "for di in xrange(test_ndoc):\n",
      "    doc = test_second[di]\n",
      "    for wi in xrange(len(doc)):\n",
      "        word = doc[wi]\n",
      "        w_no = voca[word]\n",
      "        cnt += 1\n",
      "\n",
      "        ll = 0\n",
      "        \"\"\"\n",
      "        for topic in xrange(ntopic):\n",
      "            ll += ((alpha + doc_sum[di,topic])/(alpha * ntopic + len(test_first[di]))) \\\n",
      "                * ((word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca))\n",
      "        \n",
      "        heldout_ll += np.log(ll)\n",
      "        \"\"\"\n",
      "        \n",
      "        #Jooyeon : Put log gamma for each iteration. I am not sure this is right...\n",
      "        for topic in xrange(ntopic):\n",
      "            ll += gammaln(alpha + doc_sum[di,topic]) - gammaln(alpha * ntopic + len(test_first[di])) \\\n",
      "                + gammaln(word_topic[w_no,topic] + beta) - gammaln(topic_sum[topic] + beta*nvoca)\n",
      "        heldout_ll = ll\n",
      "            \n",
      "print cnt\n",
      "print heldout_ll/cnt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "571.0\n",
        "-2.49011130396\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usually, with a general corpus, per-word heldout log likelihood ranges between -5 to -8."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}