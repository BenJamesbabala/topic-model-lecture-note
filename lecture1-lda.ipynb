{
 "metadata": {
  "name": "",
<<<<<<< HEAD
  "signature": "sha256:f1fe982ba08b7b3cd7f419e39a1e99ca8d2cf1a355fa4439bedb32f1f6e2061d"
=======
  "signature": "sha256:ce0d04a875c65961bd822d4010a7bbf8767a29e5b06384652519138abc7914a9"
>>>>>>> 44b05650b1134732046dc49e41b325ce49e77a19
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "LDA's generative processs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ntopic = 10\n",
      "nvoca = 20\n",
      "alpha = np.ones(ntopic)*0.1\n",
      "beta = np.ones(nvoca)*0.01\n",
      "theta = np.random.dirichlet(alpha)\n",
      "phi = np.random.dirichlet(beta, size=ntopic)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_z = np.random.multinomial(1, theta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
<<<<<<< HEAD
        "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])"
=======
        "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
>>>>>>> 44b05650b1134732046dc49e41b325ce49e77a19
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = _z.nonzero()[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_w = np.random.multinomial(1, phi[z,:])\n",
      "w = _w.nonzero()[0][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
<<<<<<< HEAD
        "18"
=======
        "10"
>>>>>>> 44b05650b1134732046dc49e41b325ce49e77a19
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Collapsed Gibbs Sampling\n",
      "========================\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read sample corpus from nltk.corpus.brown corpus\n",
      "# install nltk package, import nltk, and run nltk.download() to get corpora provided by nltk\n",
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "alpha = 0.1\n",
      "beta = 0.01\n",
      "\n",
      "ndoc = 500\n",
      "ntopics = 10\n",
      "\n",
      "st = set(stopwords.words())\n",
      "st.add(u'.')\n",
      "st.add(u',')\n",
      "st.add(u'\\'\\'')\n",
      "st.add(u'``')\n",
      "st.add(u':')\n",
      "st.add(u'--')\n",
      "\n",
      "_docs = brown.sents()\n",
      "docs = list()\n",
      "for di in xrange(ndoc):\n",
      "    doc = _docs[di]\n",
      "    new_doc = list()\n",
      "    for word in doc:\n",
      "        if word.lower() not in st:\n",
      "            new_doc.append(word.lower())\n",
      "    docs.append(new_doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# construct vocabulary\n",
      "_voca = set()\n",
      "for doc in docs:\n",
      "    _voca = _voca.union(set(doc))\n",
      "    \n",
      "nvoca = len(_voca)\n",
      "voca = dict()\n",
      "\n",
      "for word in _voca:\n",
      "    voca[word] = len(voca)\n",
      "voca_list = np.array(list(_voca))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set sampling variables\n",
      "\n",
      "word_topic = np.zeros([nvoca,ntopic])\n",
      "topic_sum = np.zeros(ntopic)\n",
      "doc_sum = np.zeros([ndoc, ntopic])\n",
      "\n",
      "assigned_topics = [list() for i in xrange(ndoc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#initial sampling process\n",
      "\n",
      "for di in xrange(ndoc):\n",
      "    doc = docs[di]\n",
      "    for word in doc:\n",
      "        w_no = voca[word]\n",
      "        prob = np.zeros(ntopic)\n",
      "        for topic in xrange(ntopic):\n",
      "            prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + doc_sum[di,topic])\n",
      "        prob /= np.sum(prob)\n",
      "        \n",
      "        # draw random sample\n",
      "        new_topic = np.random.multinomial(1, prob).nonzero()[0][0]\n",
      "        \n",
      "        assigned_topics[di].append(new_topic)\n",
      "        doc_sum[di,new_topic] += 1\n",
      "        topic_sum[new_topic] += 1\n",
      "        word_topic[w_no,new_topic] += 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# iterate sampling process\n",
      "\n",
      "niter = 100\n",
      "for it in xrange(niter):\n",
      "    for di in xrange(ndoc):\n",
      "        doc = docs[di]\n",
      "        for wi in xrange(len(doc)):\n",
      "            word = doc[wi]\n",
      "            w_no = voca[word]\n",
      "            prev_topic = assigned_topics[di][wi]\n",
      "\n",
      "            doc_sum[di,prev_topic] -= 1\n",
      "            topic_sum[prev_topic] -= 1\n",
      "            word_topic[w_no,prev_topic] -= 1\n",
      "\n",
      "            prob = np.zeros(ntopic)\n",
      "            for topic in xrange(ntopic):\n",
      "                prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + doc_sum[di,topic])\n",
      "            prob /= np.sum(prob)\n",
      "            \n",
      "            # draw random sample\n",
      "            new_topic = np.random.multinomial(1, prob).nonzero()[0][0]\n",
      "\n",
      "            assigned_topics[di][wi] = new_topic\n",
      "            doc_sum[di,new_topic] += 1\n",
      "            topic_sum[new_topic] += 1\n",
      "            word_topic[w_no,new_topic] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print top probability words for each topic\n",
      "for topic in xrange(ntopic):\n",
      "    print 'topic %d : %s' % (topic, ' '.join(voca_list[np.argsort(word_topic[:,topic])[::-1][0:10]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
<<<<<<< HEAD
        "topic 0 : president federal million program year new state funds grants increase\n",
        "topic 1 : texas ) ( made college j. expected told statements austin\n",
        "topic 2 : jury committee county court said election sunday fulton resolution general\n",
        "topic 3 : would said mr. one charter chairman public go school day\n",
        "topic 4 : would home law days must rule proposed board bills legislature\n",
        "topic 5 : said council last republican mr. party eisenhower night even nato\n",
        "topic 6 : house bill dallas senate passed washington sen. monday race former\n",
        "topic 7 : administration would united laos states time aid one asked state\n",
        "topic 8 : said city state take cases director two hawksley campaign persons\n",
        "topic 9 : plan department care tax 1 last medical bonds would work\n"
=======
        "topic 0 : said ( ) would issue law first work highway j.\n",
        "topic 1 : states united administration laos must party republicans secretary government another\n",
        "topic 2 : would house said committee senate president chairman public rep. passed\n",
        "topic 3 : state dallas bill administration department would bonds day make policy\n",
        "topic 4 : would year plan medical president pay grants increase million care\n",
        "topic 5 : council election cases charter vote town special possible yesterday said\n",
        "topic 6 : said mr. city made director campaign would hawksley local defense\n",
        "topic 7 : texas federal schools state school funds county provide health new\n",
        "topic 8 : jury said court fulton county city general committee since election\n",
        "topic 9 : sunday one republican night resolution monday laws asked session time\n"
>>>>>>> 44b05650b1134732046dc49e41b325ce49e77a19
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Computing Heldout Likelihood\n",
      "============================\n",
      "\n",
      "Once you infer topics from corpus, you need to validate the performance of our model. Computing heldout likelihood is the easiest way to compare your model with others. Typically, it consists of four steps:\n",
      "\n",
      "1. Prepare test set documents\n",
      "2. Split each test document into two parts\n",
      "3. Infer topic proportion of test documents from the first half by using sampling\n",
      "4. Compute heldout likelihood on the second half\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 1\n",
      "------\n",
      "Prepare test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
<<<<<<< HEAD
      "test_ndoc = 100\n",
      "test_docs = list()\n",
      "\n",
      "for di in xrange(test_ndoc):\n",
      "    doc = _docs[di]\n",
      "    new_doc = list()\n",
      "    for word in doc:\n",
      "        if word.lower() not in st:\n",
      "            new_doc.append(word.lower())\n",
      "    test_docs.append(new_doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 2\n",
      "------\n",
      "\n",
      "Split each test document into two parts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_first = list()\n",
      "test_second = list()\n",
      "for doc in test_docs:\n",
      "    first_half = list()\n",
      "    second_half = list()\n",
      "    #split it your way!\n",
      "    for wi in xrange(len(doc)):\n",
      "        word = doc[wi]\n",
      "        if voca.has_key(word):\n",
      "            if wi%2 == 0:\n",
      "                first_half.append(doc[wi])\n",
      "            else:\n",
      "                second_half.append(doc[wi])\n",
      "    test_first.append(first_half)\n",
      "    test_second.append(second_half)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 3\n",
      "------\n",
      "\n",
      "Infer topic proportion of test documents from the first half by using sampling"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# iterate sampling process\n",
      "# note that this time we do not assign new words to word_topic matrix\n",
      "\n",
      "topic_sum = np.zeros(ntopic)\n",
      "t_doc_sum = np.zeros([test_ndoc, ntopic])\n",
      "\n",
      "t_assigned_topics = [list() for i in xrange(test_ndoc)]\n",
      "\n",
      "niter = 100\n",
      "for it in xrange(niter):\n",
      "    for di in xrange(test_ndoc):\n",
      "        doc = test_first[di]\n",
      "        for wi in xrange(len(doc)):\n",
      "            word = doc[wi]\n",
      "            w_no = voca[word]\n",
      "            prev_topic = assigned_topics[di][wi]\n",
      "\n",
      "            if it != 0:\n",
      "                t_doc_sum[di,prev_topic] -= 1\n",
      "            #topic_sum[prev_topic] -= 1\n",
      "            #word_topic[w_no,prev_topic] -= 1\n",
      "\n",
      "            prob = np.zeros(ntopic)\n",
      "            for topic in xrange(ntopic):\n",
      "                prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + t_doc_sum[di,topic])\n",
      "            prob /= np.sum(prob)\n",
      "\n",
      "            # draw random sample\n",
      "            new_topic = np.random.multinomial(1, prob).nonzero()[0][0]\n",
      "\n",
      "            assigned_topics[di][wi] = new_topic\n",
      "            t_doc_sum[di,new_topic] += 1\n",
      "            #topic_sum[new_topic] += 1\n",
      "            #word_topic[w_no,new_topic] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 4\n",
      "------\n",
      "\n",
      "Compute per-word heldout log likelihood on the second half"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heldout_ll = 0.\n",
      "cnt = 0.\n",
      "\n",
      "for di in xrange(test_ndoc):\n",
      "    doc = test_second[di]\n",
      "    for wi in xrange(len(doc)):\n",
      "        word = doc[wi]\n",
      "        w_no = voca[word]\n",
      "        cnt += 1\n",
      "\n",
      "        ll = 0\n",
      "        for topic in xrange(ntopic):\n",
      "            ll += ((alpha + doc_sum[di,topic])/(alpha * ntopic + len(test_first[di]))) \\\n",
      "                * ((word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca))\n",
      "        \n",
      "        heldout_ll += np.log(ll)\n",
      "            \n",
      "print cnt\n",
      "print heldout_ll/cnt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "571.0\n",
        "-2.49011130396\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usually, with a general corpus, per-word heldout log likelihood ranges between -5 to -8."
     ]
=======
      "# or removing initialization steps using conditional statement\n",
      "\n",
      "niter = 100\n",
      "for it in xrange(niter):\n",
      "    for di in xrange(ndoc):\n",
      "        doc = docs[di]\n",
      "        for wi in xrange(len(doc)):\n",
      "            word = doc[wi]\n",
      "            w_no = voca[word]\n",
      "            \n",
      "            if it != 0:\n",
      "                prev_topic = assigned_topics[di][wi]\n",
      "\n",
      "                doc_sum[di,prev_topic] -= 1\n",
      "                topic_sum[prev_topic] -= 1\n",
      "                word_topic[w_no,prev_topic] -= 1\n",
      "\n",
      "            prob = np.zeros(ntopic)\n",
      "            for topic in xrange(ntopic):\n",
      "                prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + doc_sum[di,topic])\n",
      "            prob /= np.sum(prob)\n",
      "            \n",
      "            # draw random sample\n",
      "            new_topic = np.random.multinomial(1, prob).nonzero()[0][0]\n",
      "\n",
      "            assigned_topics[di][wi] = new_topic\n",
      "            doc_sum[di,new_topic] += 1\n",
      "            topic_sum[new_topic] += 1\n",
      "            word_topic[w_no,new_topic] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print top probability words for each topic\n",
      "for topic in xrange(ntopic):\n",
      "    print 'topic %d : %s' % (topic, ' '.join(voca_list[np.argsort(word_topic[:,topic])[::-1][0:10]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 0 : said would ( ) issue law highway work first legislators\n",
        "topic 1 : states united administration laos must party republicans secretary government another\n",
        "topic 2 : house would said president committee senate chairman public bill resolution\n",
        "topic 3 : state dallas administration bill would bonds department day make policy\n",
        "topic 4 : would year plan medical president pay home million care grants\n",
        "topic 5 : council election cases charter vote special town said possible yesterday\n",
        "topic 6 : said mr. city made campaign director hawksley would local told\n",
        "topic 7 : texas state federal schools school funds county new provide program\n",
        "topic 8 : jury said court fulton general county city committee since take\n",
        "topic 9 : sunday one republican night monday asked time laws session primary\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
>>>>>>> 44b05650b1134732046dc49e41b325ce49e77a19
    }
   ],
   "metadata": {}
  }
 ]
}