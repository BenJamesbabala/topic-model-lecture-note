{
 "metadata": {
  "name": "",
  "signature": "sha256:e8067873ca2d5cac41884098741421bbc74678a5e7bb7d9f8ea530b215781c6b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "LDA's generative processs"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ntopic = 10\n",
      "nvoca = 20\n",
      "alpha = np.ones(ntopic)*0.1\n",
      "beta = np.ones(nvoca)*0.01\n",
      "theta = np.random.dirichlet(alpha)\n",
      "phi = np.random.dirichlet(beta, size=ntopic)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_z = np.random.multinomial(1, theta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_z"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = _z.argmax()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_w = np.random.multinomial(1, phi[z,:])\n",
      "w = _w.argmax()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "5"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Collapsed Gibbs Sampling\n",
      "========================\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read sample corpus from nltk.corpus.brown corpus\n",
      "# install nltk package, import nltk, and run nltk.download() to get corpora provided by nltk\n",
      "import nltk\n",
      "from nltk.corpus import brown\n",
      "from nltk.corpus import stopwords\n",
      "from scipy.special import gammaln\n",
      "\n",
      "alpha = 0.1\n",
      "beta = 0.01\n",
      "\n",
      "ndoc = 500\n",
      "ntopics = 10\n",
      "\n",
      "st = set(stopwords.words())\n",
      "st.add(u'.')\n",
      "st.add(u',')\n",
      "st.add(u'\\'\\'')\n",
      "st.add(u'``')\n",
      "st.add(u':')\n",
      "st.add(u'--')\n",
      "\n",
      "_docs = brown.sents()\n",
      "docs = list()\n",
      "for di in xrange(ndoc):\n",
      "    doc = _docs[di]\n",
      "    new_doc = list()\n",
      "    for word in doc:\n",
      "        if word.lower() not in st:\n",
      "            new_doc.append(word.lower())\n",
      "    docs.append(new_doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# construct vocabulary\n",
      "_voca = set()\n",
      "for doc in docs:\n",
      "    _voca = _voca.union(set(doc))\n",
      "    \n",
      "nvoca = len(_voca)\n",
      "voca = dict()\n",
      "\n",
      "for word in _voca:\n",
      "    voca[word] = len(voca)\n",
      "voca_list = np.array(list(_voca))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set sampling variables\n",
      "\n",
      "word_topic = np.zeros([nvoca,ntopic])\n",
      "topic_sum = np.zeros(ntopic)\n",
      "doc_sum = np.zeros([ndoc, ntopic])\n",
      "\n",
      "assigned_topics = [list() for i in xrange(ndoc)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#initial sampling process\n",
      "\n",
      "for di in xrange(ndoc):\n",
      "    doc = docs[di]\n",
      "    for word in doc:\n",
      "        w_no = voca[word]\n",
      "        prob = np.zeros(ntopic)\n",
      "        for topic in xrange(ntopic):\n",
      "            prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + doc_sum[di,topic])\n",
      "        prob /= np.sum(prob)\n",
      "        \n",
      "        # draw random sample\n",
      "        new_topic = np.random.multinomial(1, prob).argmax()\n",
      "        \n",
      "        assigned_topics[di].append(new_topic)\n",
      "        doc_sum[di,new_topic] += 1\n",
      "        topic_sum[new_topic] += 1\n",
      "        word_topic[w_no,new_topic] += 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# iterate sampling process\n",
      "\n",
      "niter = 100\n",
      "for it in xrange(niter):\n",
      "    for di in xrange(ndoc):\n",
      "        doc = docs[di]\n",
      "        for wi in xrange(len(doc)):\n",
      "            word = doc[wi]\n",
      "            w_no = voca[word]\n",
      "            prev_topic = assigned_topics[di][wi]\n",
      "\n",
      "            doc_sum[di,prev_topic] -= 1\n",
      "            topic_sum[prev_topic] -= 1\n",
      "            word_topic[w_no,prev_topic] -= 1\n",
      "\n",
      "            prob = np.zeros(ntopic)\n",
      "            for topic in xrange(ntopic):\n",
      "                prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + doc_sum[di,topic])\n",
      "            prob /= np.sum(prob)\n",
      "            \n",
      "            # draw random sample\n",
      "            new_topic = np.random.multinomial(1, prob).argmax()\n",
      "\n",
      "            assigned_topics[di][wi] = new_topic\n",
      "            doc_sum[di,new_topic] += 1\n",
      "            topic_sum[new_topic] += 1\n",
      "            word_topic[w_no,new_topic] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print top probability words for each topic\n",
      "for topic in xrange(ntopic):\n",
      "    print 'topic %d : %s' % (topic, ' '.join(voca_list[np.argsort(word_topic[:,topic])[::-1][0:10]]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "topic 0 : would year president pay medical ) ( care schools texas\n",
        "topic 1 : said jury federal county program fulton election funds health president\n",
        "topic 2 : mr. charter last campaign council night town election yesterday group\n",
        "topic 3 : administration laos cases persons policy another ward policies karns involved\n",
        "topic 4 : million state dallas texas 1 new dollars department bonds hospital\n",
        "topic 5 : said committee bill senate party public house republican passed state\n",
        "topic 6 : made court general said governor two highway work expected laws\n",
        "topic 7 : states united state since would nato taken secretary get property\n",
        "topic 8 : said resolution house home days vote day sunday adc board\n",
        "topic 9 : city said would mr. hawksley college one dr. aid back\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Computing Heldout Likelihood\n",
      "============================\n",
      "\n",
      "Once you infer topics from corpus, you need to validate the performance of our model. Computing heldout likelihood is the easiest way to compare your model with others. Typically, it consists of four steps:\n",
      "\n",
      "1. Prepare test set documents\n",
      "2. Split each test document into two parts\n",
      "3. Infer topic proportion of test documents from the first half by using sampling\n",
      "4. Compute heldout likelihood on the second half\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 1\n",
      "------\n",
      "Prepare test set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_ndoc = 100\n",
      "test_docs = list()\n",
      "\n",
      "for di in xrange(test_ndoc):\n",
      "    doc = _docs[di]\n",
      "    new_doc = list()\n",
      "    for word in doc:\n",
      "        if word.lower() not in st:\n",
      "            new_doc.append(word.lower())\n",
      "    test_docs.append(new_doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 2\n",
      "------\n",
      "\n",
      "Split each test document into two parts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_first = list()\n",
      "test_second = list()\n",
      "for doc in test_docs:\n",
      "    first_half = list()\n",
      "    second_half = list()\n",
      "    #split it your way!\n",
      "    for wi in xrange(len(doc)):\n",
      "        word = doc[wi]\n",
      "        if voca.has_key(word):\n",
      "            if wi%2 == 0:\n",
      "                first_half.append(doc[wi])\n",
      "            else:\n",
      "                second_half.append(doc[wi])\n",
      "    test_first.append(first_half)\n",
      "    test_second.append(second_half)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 3\n",
      "------\n",
      "\n",
      "Infer topic proportion of test documents from the first half by using sampling"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# iterate sampling process\n",
      "# note that this time we do not assign new words to word_topic matrix\n",
      "\n",
      "#topic_sum = np.zeros(ntopic) # Jooyeon : topic_sum is identical for training and testing\n",
      "t_doc_sum = np.zeros([test_ndoc, ntopic])\n",
      "\n",
      "t_assigned_topics = [[0 for j in xrange(len(test_first[i]))]for i in xrange(test_ndoc)]\n",
      "\n",
      "# Jooyeon : t_doc_sum and t_assigned_topics need to be initialized.\n",
      "\n",
      "niter = 100\n",
      "for it in xrange(niter):\n",
      "    for di in xrange(test_ndoc):\n",
      "        doc = test_first[di]\n",
      "        for wi in xrange(len(doc)):\n",
      "            word = doc[wi]\n",
      "            w_no = voca[word]\n",
      "            #prev_topic = assigned_topics[di][wi]\n",
      "            prev_topic = t_assigned_topics[di][wi]  # Jooyeon : changed to t_assigned_topics\n",
      "\n",
      "            if it != 0:\n",
      "                t_doc_sum[di,prev_topic] -= 1\n",
      "            #topic_sum[prev_topic] -= 1\n",
      "            #word_topic[w_no,prev_topic] -= 1\n",
      "\n",
      "            prob = np.zeros(ntopic)\n",
      "            for topic in xrange(ntopic):\n",
      "                prob[topic] = (word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca)*(alpha + t_doc_sum[di,topic])\n",
      "            prob /= np.sum(prob)\n",
      "\n",
      "            # draw random sample\n",
      "            new_topic = np.random.multinomial(1, prob).argmax()\n",
      "\n",
      "            #assigned_topics[di][wi] = new_topic\n",
      "            t_assigned_topics[di][wi] = new_topic # Jooyeon : changed to t_assigned_topics\n",
      "            t_doc_sum[di,new_topic] += 1\n",
      "            #topic_sum[new_topic] += 1\n",
      "            #word_topic[w_no,new_topic] += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Step 4\n",
      "------\n",
      "\n",
      "Compute per-word heldout log likelihood on the second half"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "heldout_ll = 0.\n",
      "cnt = 0.\n",
      "\n",
      "for di in xrange(test_ndoc):\n",
      "    doc = test_second[di]\n",
      "    for wi in xrange(len(doc)):\n",
      "        word = doc[wi]\n",
      "        w_no = voca[word]\n",
      "        cnt += 1\n",
      "\n",
      "        # see A Collapsed Variational Bayesian Inference\n",
      "        # Algorithm for Latent Dirichlet Allocation by Teh & et al (2006)\n",
      "        # for how to compute the heldout likelihood\n",
      "        ll = 0\n",
      "        for topic in xrange(ntopic):\n",
      "            ll += ((alpha + doc_sum[di,topic])/(alpha * ntopic + len(test_first[di]))) \\\n",
      "                * ((word_topic[w_no,topic] + beta)/(topic_sum[topic] + beta*nvoca))\n",
      "        \n",
      "        heldout_ll += np.log(ll)\n",
      "            \n",
      "print cnt\n",
      "print heldout_ll/cnt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "571.0\n",
        "-5.65557797348\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usually, with a general corpus, per-word heldout log likelihood ranges between -5 to -8."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gibbs Sampling with PyMC\n",
      "========================\n",
      "Typically, MCMC method with naive python is more than 100 times slower than the same implementation written in compiled language languages such as c, and java. In this section, we will see the performance of pymc package as an alternative approach.\n",
      "\n",
      "I tested LDA with PyMC below, but it was extremely slow. Even it didn't work with 500 documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert word list to vector\n",
      "word_ids = list()   # word appearance\n",
      "word_cnt = list()   # word count\n",
      "for doc in docs:\n",
      "    ids = np.zeros(nvoca, dtype=int)\n",
      "    cnt = np.zeros(nvoca, dtype=int)\n",
      "    for word in doc:\n",
      "        ids[voca[word]] = 1\n",
      "        cnt[voca[word]] += 1\n",
      "    word_ids.append(ids)\n",
      "    word_cnt.append(cnt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# implementation of LDA with pymc\n",
      "# original source code from http://stats.stackexchange.com/questions/104771/latent-dirichlet-allocation-in-pymc\n",
      "\n",
      "import numpy as np\n",
      "import pymc as pm\n",
      "\n",
      "K = ntopic # number of topics\n",
      "V = nvoca # number of words\n",
      "D = 10 # number of documents\n",
      "\n",
      "data = np.array(word_cnt)\n",
      "\n",
      "alpha = np.ones(K)\n",
      "beta = np.ones(V)\n",
      "\n",
      "theta = pm.Container([pm.CompletedDirichlet(\"theta_%s\" % i, pm.Dirichlet(\"ptheta_%s\" % i, theta=alpha)) for i in range(D)])\n",
      "phi = pm.Container([pm.CompletedDirichlet(\"phi_%s\" % k, pm.Dirichlet(\"pphi_%s\" % k, theta=beta)) for k in range(K)])\n",
      "Wd = [len(doc) for doc in data]\n",
      "\n",
      "z = pm.Container([pm.Categorical('z_%i' % d, \n",
      "                     p = theta[d], \n",
      "                     size=Wd[d],\n",
      "                     value=np.random.randint(K, size=Wd[d]))\n",
      "                  for d in range(D)])\n",
      "\n",
      "# cannot use p=phi[z[d][i]] here since phi is an ordinary list while z[d][i] is stochastic\n",
      "w = pm.Container([pm.Categorical(\"w_%i_%i\" % (d,i),\n",
      "                    p = pm.Lambda('phi_z_%i_%i' % (d,i), \n",
      "                              lambda z=z[d][i], phi=phi: phi[z]),\n",
      "                    value=data[d][i], \n",
      "                    observed=True)\n",
      "                  for d in range(D) for i in range(Wd[d])])\n",
      "\n",
      "model = pm.Model([theta, phi, z, w])\n",
      "mcmc = pm.MCMC(model)\n",
      "mcmc.sample(100)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Couldn't import dot_parser, loading of dot files will not be possible.\n",
        "\r"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [                  2%                  ] 2 of 100 complete in 0.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-                 3%                  ] 3 of 100 complete in 1.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-                 4%                  ] 4 of 100 complete in 2.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-                 5%                  ] 5 of 100 complete in 2.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [--                6%                  ] 6 of 100 complete in 3.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [--                7%                  ] 7 of 100 complete in 4.0 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---               8%                  ] 8 of 100 complete in 4.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---               9%                  ] 9 of 100 complete in 5.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---              10%                  ] 10 of 100 complete in 6.0 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [----             11%                  ] 11 of 100 complete in 6.6 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [----             12%                  ] 12 of 100 complete in 7.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [----             13%                  ] 13 of 100 complete in 7.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----            14%                  ] 14 of 100 complete in 8.5 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----            15%                  ] 15 of 100 complete in 9.0 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [------           16%                  ] 16 of 100 complete in 9.6 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [------           17%                  ] 17 of 100 complete in 10.2 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [------           18%                  ] 18 of 100 complete in 10.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-------          19%                  ] 19 of 100 complete in 11.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-------          20%                  ] 20 of 100 complete in 12.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-------          21%                  ] 21 of 100 complete in 12.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [--------         22%                  ] 22 of 100 complete in 13.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [--------         23%                  ] 23 of 100 complete in 14.0 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---------        24%                  ] 24 of 100 complete in 14.6 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---------        25%                  ] 25 of 100 complete in 15.2 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---------        26%                  ] 26 of 100 complete in 15.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [----------       27%                  ] 27 of 100 complete in 16.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [----------       28%                  ] 28 of 100 complete in 16.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------      29%                  ] 29 of 100 complete in 17.5 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------      30%                  ] 30 of 100 complete in 18.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------      31%                  ] 31 of 100 complete in 18.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [------------     32%                  ] 32 of 100 complete in 19.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [------------     33%                  ] 33 of 100 complete in 19.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [------------     34%                  ] 34 of 100 complete in 20.6 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-------------    35%                  ] 35 of 100 complete in 21.2 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-------------    36%                  ] 36 of 100 complete in 21.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [--------------   37%                  ] 37 of 100 complete in 22.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [--------------   38%                  ] 38 of 100 complete in 23.0 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [--------------   39%                  ] 39 of 100 complete in 23.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---------------  40%                  ] 40 of 100 complete in 24.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---------------  41%                  ] 41 of 100 complete in 24.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---------------  42%                  ] 42 of 100 complete in 25.5 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---------------- 43%                  ] 43 of 100 complete in 26.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [---------------- 44%                  ] 44 of 100 complete in 26.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------45%                  ] 45 of 100 complete in 27.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------46%                  ] 46 of 100 complete in 27.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------47%                  ] 47 of 100 complete in 28.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------48%                  ] 48 of 100 complete in 29.0 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------49%                  ] 49 of 100 complete in 29.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------50%                  ] 50 of 100 complete in 30.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------51%                  ] 51 of 100 complete in 31.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------52%                  ] 52 of 100 complete in 31.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------53%                  ] 53 of 100 complete in 32.6 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------54%                  ] 54 of 100 complete in 33.5 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------55%                  ] 55 of 100 complete in 34.2 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------56%-                 ] 56 of 100 complete in 35.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------57%-                 ] 57 of 100 complete in 35.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------58%--                ] 58 of 100 complete in 36.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------59%--                ] 59 of 100 complete in 37.6 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------60%--                ] 60 of 100 complete in 38.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------61%---               ] 61 of 100 complete in 38.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------62%---               ] 62 of 100 complete in 39.5 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------63%---               ] 63 of 100 complete in 40.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------64%----              ] 64 of 100 complete in 40.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------65%----              ] 65 of 100 complete in 41.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------66%-----             ] 66 of 100 complete in 42.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------67%-----             ] 67 of 100 complete in 42.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------68%-----             ] 68 of 100 complete in 43.2 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------69%------            ] 69 of 100 complete in 43.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------70%------            ] 70 of 100 complete in 44.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------71%------            ] 71 of 100 complete in 45.0 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------72%-------           ] 72 of 100 complete in 45.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------73%-------           ] 73 of 100 complete in 46.2 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------74%--------          ] 74 of 100 complete in 46.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------75%--------          ] 75 of 100 complete in 47.6 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------76%--------          ] 76 of 100 complete in 48.2 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------77%---------         ] 77 of 100 complete in 48.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------78%---------         ] 78 of 100 complete in 49.6 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------79%----------        ] 79 of 100 complete in 50.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------80%----------        ] 80 of 100 complete in 50.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------81%----------        ] 81 of 100 complete in 51.5 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------82%-----------       ] 82 of 100 complete in 52.2 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------83%-----------       ] 83 of 100 complete in 52.8 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------84%-----------       ] 84 of 100 complete in 53.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------85%------------      ] 85 of 100 complete in 53.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------86%------------      ] 86 of 100 complete in 54.5 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------87%-------------     ] 87 of 100 complete in 55.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------88%-------------     ] 88 of 100 complete in 55.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------89%-------------     ] 89 of 100 complete in 56.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------90%--------------    ] 90 of 100 complete in 57.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------91%--------------    ] 91 of 100 complete in 57.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------92%--------------    ] 92 of 100 complete in 58.3 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------93%---------------   ] 93 of 100 complete in 58.9 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------94%---------------   ] 94 of 100 complete in 59.5 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------95%----------------  ] 95 of 100 complete in 60.1 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------96%----------------  ] 96 of 100 complete in 60.7 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------97%----------------  ] 97 of 100 complete in 61.4 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------98%----------------- ] 98 of 100 complete in 62.0 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------99%----------------- ] 99 of 100 complete in 62.6 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------100%-----------------] 100 of 100 complete in 63.2 sec"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " [-----------------101%-----------------] 101 of 100 complete in 63.8 sec"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    }
   ],
   "metadata": {}
  }
 ]
}